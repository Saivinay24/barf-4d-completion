# BARF: Generative 4D Completion

**Transform monocular video into complete, explorable 360Â° 4D experiences.**

## What is BARF?

Current 4D reconstruction methods (like Google D4RT, NeoVerse, 4DGaussians) can turn a phone video into a 3D scene... but **only what the camera saw**. Film someone from the front? The back of their head is **empty**. You can't walk behind them in VR.

**BARF (Binarily Augmented Reality Footage)** fills those gaps using generative AI, creating a complete 360Â° 4D reconstruction you can freely explore.

## The Problem We're Solving

| Current SOTA (D4RT, 4DGS) | BARF (Our Solution) |
|---------------------------|---------------------|
| Reconstructs visible surfaces only | Generates missing back-sides with AI |
| ~180Â° field of view from camera | Complete 360Â° explorable scene |
| Empty gaps when you rotate around | Temporally consistent fills |
| Can't "walk behind" objects in VR | Full free-viewpoint navigation |

## Architecture

```
Input: Monocular video (1 camera angle)
   â†“
[1] 4D Reconstruction (4DGaussians / D4RT)
   â†“
[2] Gap Detection (voxel-based)
   â†“
[3] Novel View Generation (Zero123++ / SV3D)
   â†“
[4] Temporal Consistency (sliding denoising) â† OUR INNOVATION
   â†“
Output: Complete 360Â° 4D scene
```

## Team Structure (Feb 14-28, 2026 Sprint)

| Role | Person | Focus |
|------|--------|-------|
| ğŸ¯ **Vinay (R0)** | Lead | Architecture + Integration + Temporal Consistency |
| ğŸ“š **Shrit (R1)** | Repo Hunter | Run CAT4D/Vivid4D/NeoVerse, benchmark |
| ğŸ“¦ **Aditya (R2)** | Data Engineer | DAVIS/Sintel + Depth Maps + COLMAP |
| ğŸ” **Aryan (R3)** | 4D Reconstruction | Run 4DGaussians, produce partial 4D |
| ğŸ¨ **Tanisha (R4)** | Novel View Gen | Zero123++/SV3D for back-view generation |
| ğŸ–¥ï¸ **Palak (R5)** | Viewer Engineer | Web viewer with gap visualization |

## Repository Structure

```
barf-4d-completion/
â”œâ”€â”€ tasks/                  # Individual task assignments (READ YOUR FILE!)
â”‚   â”œâ”€â”€ 00_README.md       # Project overview
â”‚   â”œâ”€â”€ R1_repo_hunter_shrit.md  # Shrit's tasks
â”‚   â”œâ”€â”€ R2_data_engineer_aditya.md # Aditya's tasks
â”‚   â”œâ”€â”€ R3_4d_reconstruction_aryan.md # Aryan's tasks
â”‚   â”œâ”€â”€ R4_novel_view_generator_tanisha.md # Tanisha's tasks
â”‚   â””â”€â”€ R5_viewer_engineer_palak.md # Palak's tasks
â”‚
â”œâ”€â”€ research/              # Shrit: benchmark outputs, comparisons
â”œâ”€â”€ datasets/              # Aditya: DAVIS, Sintel, depth maps, COLMAP
â”œâ”€â”€ reconstructions/       # Aryan: 4D point clouds, gap detection
â”œâ”€â”€ diffusion_experiments/ # Tanisha: generated views, consistency tests
â”œâ”€â”€ viewer/                # Palak: web-based 3D viewer
â”œâ”€â”€ vr_viewer/             # (Future) VR viewer for Quest
â””â”€â”€ core/                  # Vinay: integration pipeline, temporal consistency
```

## Quick Start

### For Team Members

1. **Read your task file:** `tasks/[YOUR_NAME]_[ROLE].md`
2. **Clone this repo:**
   ```bash
   git clone https://github.com/Saivinay24/barf-4d-completion
   cd barf-4d-completion
   ```
3. **Work in YOUR folder only** (to avoid conflicts)
4. **Push daily:**
   ```bash
   git add [YOUR_FOLDER]/
   git commit -m "[YOUR_NAME]: what you did today"
   git pull
   git push
   ```

### Git Workflow Rules

- âœ… **DO:** Only edit files in your assigned folder
- âœ… **DO:** Commit at end of each day with descriptive messages
- âœ… **DO:** Pull before pushing to get others' updates
- âŒ **DON'T:** Edit other people's folders (ask first)
- âŒ **DON'T:** Commit large binary files (use Git LFS or Drive)

## Timeline

**Week 1 (Feb 14-21):** Clone repos, run SOTA methods, produce outputs  
**Week 2 (Feb 22-28):** Integration, benchmarking, demo prep  
**Feb 28:** Final demo presentation

## Tech Stack

- **4D Reconstruction:** 4DGaussians, Shape-of-Motion, (future: D4RT API)
- **Depth Estimation:** Depth Anything V2
- **Camera Poses:** COLMAP
- **Novel View Synthesis:** Zero123++, SV3D, Stable Video Diffusion
- **3D Viewing:** Three.js, antimatter15/splat viewer
- **Temporal Consistency:** Optical flow (RAFT), sliding denoising

## Key Deliverables (Feb 28)

1. Working end-to-end pipeline (video â†’ complete 4D)
2. Benchmark comparison vs CAT4D/Vivid4D/NeoVerse
3. Web viewer showing before/after gap filling
4. Quantitative metrics (gap coverage %, temporal consistency)
5. Demo video + presentation

## Research Questions We're Tackling

1. **Can diffusion models generate plausible back-views from monocular video?**
2. **How do we maintain temporal consistency across generated frames?**
3. **What's the gap between reconstruction-only vs generative completion?**
4. **Is this fast enough for practical VR applications?**

## Future Work (Post-Sprint)

- VR integration (Meta Quest) once hardware arrives
- Real-time optimization for 90 FPS VR
- D4RT API integration when released (mid-2026)
- Explore business models: SaaS, API, plugin marketplace

## Contact

**Lead:** Vinay (Saivinay24)  
**Project Duration:** Feb 14 - Feb 28, 2026  
**License:** TBD (likely MIT for research components)

---

**Philosophy:** Don't build. Fork. Run. Produce.
